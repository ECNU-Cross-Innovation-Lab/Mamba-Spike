"""
äº‘ç«¯GPUè®­ç»ƒè„šæœ¬ - é’ˆå¯¹NVIDIA GPU (RTX 3090/4090/5090, A100ç­‰) ä¼˜åŒ–
æ”¯æŒCUDAåŠ é€Ÿï¼Œæ€§èƒ½è¿œè¶…æœ¬åœ°è®­ç»ƒ
"""

import argparse
import sys
import torch

def check_cuda():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("=" * 70)
    print("ğŸš€ äº‘ç«¯GPUè®­ç»ƒé…ç½®æ£€æµ‹")
    print("=" * 70)

    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨!")
        print("\nå¯èƒ½åŸå› :")
        print("  1. æ²¡æœ‰NVIDIA GPU")
        print("  2. CUDAé©±åŠ¨æœªå®‰è£…")
        print("  3. PyTorchæœªå®‰è£…CUDAç‰ˆæœ¬")
        print("\nè§£å†³æ–¹æ¡ˆ:")
        print("  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        sys.exit(1)

    print(f"âœ… CUDAå¯ç”¨")
    print(f"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")

    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"\nğŸ“Š GPU {i}: {props.name}")
        print(f"   æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
        print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"   å¤šå¤„ç†å™¨: {props.multi_processor_count}")

    print("=" * 70)
    return True


def get_optimized_config(gpu_name, dataset):
    """æ ¹æ®GPUå‹å·å’Œæ•°æ®é›†è¿”å›ä¼˜åŒ–é…ç½®"""

    # GPUé…ç½®æ˜ å°„
    gpu_configs = {
        'RTX 5090': {'batch_size': 128, 'workers': 8, 'note': 'ğŸ”¥ é¡¶çº§æ€§èƒ½'},
        'RTX 4090': {'batch_size': 96, 'workers': 8, 'note': 'âš¡ æé«˜æ€§èƒ½'},
        'RTX 3090': {'batch_size': 64, 'workers': 6, 'note': 'ğŸ’ª é«˜æ€§èƒ½'},
        'A100': {'batch_size': 128, 'workers': 8, 'note': 'ğŸ† ä¸“ä¸šçº§'},
        'V100': {'batch_size': 64, 'workers': 6, 'note': 'ğŸ¯ é«˜æ€§èƒ½'},
        'T4': {'batch_size': 32, 'workers': 4, 'note': 'âœ… è‰¯å¥½æ€§èƒ½'},
    }

    # æ£€æµ‹GPUå‹å·
    config = {'batch_size': 32, 'workers': 4, 'note': 'âœ“ æ ‡å‡†é…ç½®'}

    for key in gpu_configs:
        if key.lower() in gpu_name.lower():
            config = gpu_configs[key]
            break

    return config


def estimate_training_time(gpu_name, dataset, epochs):
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""

    # æ—¶é—´ä¼°ç®—ï¼ˆåˆ†é’Ÿ/100 epochsï¼‰
    time_estimates = {
        'RTX 5090': {'nmnist': 15, 'dvsgesture': 30, 'cifar10dvs': 45},
        'RTX 4090': {'nmnist': 20, 'dvsgesture': 40, 'cifar10dvs': 60},
        'RTX 3090': {'nmnist': 35, 'dvsgesture': 70, 'cifar10dvs': 90},
        'A100': {'nmnist': 18, 'dvsgesture': 35, 'cifar10dvs': 50},
        'V100': {'nmnist': 40, 'dvsgesture': 80, 'cifar10dvs': 100},
        'T4': {'nmnist': 90, 'dvsgesture': 180, 'cifar10dvs': 240},
    }

    # æŸ¥æ‰¾åŒ¹é…çš„GPU
    base_time = 120  # é»˜è®¤æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
    for key in time_estimates:
        if key.lower() in gpu_name.lower():
            if dataset in time_estimates[key]:
                base_time = time_estimates[key][dataset]
            break

    # æ ¹æ®epochsè°ƒæ•´
    estimated_minutes = base_time * (epochs / 100)

    if estimated_minutes < 60:
        return f"{estimated_minutes:.0f} åˆ†é’Ÿ"
    else:
        hours = estimated_minutes / 60
        return f"{hours:.1f} å°æ—¶"


def main():
    """äº‘ç«¯GPUè®­ç»ƒä¸»å‡½æ•°"""

    # æ£€æŸ¥CUDA
    check_cuda()

    parser = argparse.ArgumentParser(description='äº‘ç«¯GPUè®­ç»ƒ - NVIDIA CUDAåŠ é€Ÿ')

    # Dataset
    parser.add_argument('--dataset', type=str, default='nmnist',
                        choices=['nmnist', 'dvsgesture', 'cifar10dvs'],
                        help='æ•°æ®é›†é€‰æ‹©')

    # Training parameters (é’ˆå¯¹é«˜æ€§èƒ½GPUä¼˜åŒ–)
    parser.add_argument('--epochs', type=int, default=100,
                        help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤100ï¼‰')
    parser.add_argument('--batch-size', type=int, default=None,
                        help='æ‰¹æ¬¡å¤§å°ï¼ˆç•™ç©ºè‡ªåŠ¨é€‰æ‹©ï¼‰')
    parser.add_argument('--lr', type=float, default=1e-3,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--weight-decay', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡')
    parser.add_argument('--clip-grad', type=float, default=1.0,
                        help='æ¢¯åº¦è£å‰ª')
    parser.add_argument('--num-workers', type=int, default=None,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼ˆç•™ç©ºè‡ªåŠ¨é€‰æ‹©ï¼‰')

    # Dataset parameters
    parser.add_argument('--time-window', type=int, default=300000,
                        help='æ—¶é—´çª—å£ï¼ˆå¾®ç§’ï¼‰')
    parser.add_argument('--dt', type=float, default=1000,
                        help='æ—¶é—´åˆ†è¾¨ç‡ï¼ˆå¾®ç§’ï¼‰')

    # System
    parser.add_argument('--data-dir', type=str, default='./data',
                        help='æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', type=str, default='./outputs',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­')

    args = parser.parse_args()

    # è·å–GPUä¿¡æ¯
    gpu_name = torch.cuda.get_device_name(0)

    # è‡ªåŠ¨é…ç½®æœ€ä¼˜å‚æ•°
    config = get_optimized_config(gpu_name, args.dataset)

    if args.batch_size is None:
        args.batch_size = config['batch_size']

    if args.num_workers is None:
        args.num_workers = config['workers']

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("\nğŸ’¡ äº‘ç«¯GPUè®­ç»ƒé…ç½®")
    print("=" * 70)
    print(f"  GPU: {gpu_name} {config['note']}")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size} (GPUä¼˜åŒ–)")
    print(f"  å­¦ä¹ ç‡: {args.lr}")
    print(f"  æ•°æ®åŠ è½½çº¿ç¨‹: {args.num_workers}")
    print("=" * 70)

    # ä¼°ç®—è®­ç»ƒæ—¶é—´
    est_time = estimate_training_time(gpu_name, args.dataset, args.epochs)
    print(f"\nâ±ï¸  é¢„è®¡è®­ç»ƒæ—¶é—´: {est_time}")
    print("=" * 70)

    # æ€§èƒ½æç¤º
    print("\nğŸš€ æ€§èƒ½ä¼˜åŒ–æç¤º:")
    print("  âœ“ ä½¿ç”¨å¤§batch sizeå……åˆ†åˆ©ç”¨GPU")
    print("  âœ“ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¦‚æœæ”¯æŒï¼‰")
    print("  âœ“ ä½¿ç”¨å¤šçº¿ç¨‹æ•°æ®åŠ è½½")
    print("  âœ“ ç¡®ä¿æ•°æ®åœ¨SSDä¸Šä»¥åŠ å¿«åŠ è½½é€Ÿåº¦")
    print("=" * 70)

    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print("=" * 70)

    # å¯¼å…¥è®­ç»ƒæ¨¡å—
    from train import Trainer
    import numpy as np

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    torch.cuda.manual_seed(args.seed)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(args)

    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"å·²ä¿å­˜çš„checkpointå¯ä»¥åœ¨ {args.output_dir} æ‰¾åˆ°")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "=" * 70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)

    # æ˜¾ç¤ºåç»­æ­¥éª¤
    print("\nğŸ“Š åç»­æ­¥éª¤:")
    print(f"1. æŸ¥çœ‹TensorBoard: tensorboard --logdir={args.output_dir}")
    print(f"2. è¯„ä¼°æ¨¡å‹: python evaluate.py --checkpoint {args.output_dir}/.../checkpoint_best.pth")
    print(f"3. ä¸‹è½½ç»“æœ: æ‰“åŒ…outputsç›®å½•")
    print("=" * 70)


if __name__ == "__main__":
    main()
